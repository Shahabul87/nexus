{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# NEXUS: Bilirubin Regression from MedSigLIP Embeddings\n",
    "\n",
    "**Novel Task**: Predict continuous bilirubin levels (mg/dL) from neonatal skin images.\n",
    "\n",
    "## Why This Is Novel\n",
    "MedSigLIP was trained for medical image-text similarity. Using its frozen embeddings\n",
    "for **continuous bilirubin regression** is a genuinely novel application that goes\n",
    "beyond its original zero-shot classification design.\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "Neonatal skin image\n",
    "       |\n",
    "  [Frozen MedSigLIP encoder]  (google/medsiglip-448)\n",
    "       |\n",
    "  1152-dim embedding\n",
    "       |\n",
    "  [Linear(1152, 256) -> ReLU -> Dropout(0.3) -> Linear(256, 1)]\n",
    "       |\n",
    "  Predicted bilirubin (mg/dL)\n",
    "```\n",
    "\n",
    "## Dataset\n",
    "- **NeoJaundice**: 2,235 neonatal images with ground truth serum bilirubin (mg/dL)\n",
    "- Split: 70% train / 15% val / 15% test\n",
    "\n",
    "## HAI-DEF Model\n",
    "- **MedSigLIP** (`google/medsiglip-448`) — frozen vision encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup & Imports\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, '../src')\n",
    "sys.path.insert(0, '../scripts/training')\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load & Explore the NeoJaundice Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data/raw/neojaundice\")\n",
    "CSV_PATH = DATA_DIR / \"chd_jaundice_published_2.csv\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nBilirubin statistics:\")\n",
    "print(df[\"blood(mg/dL)\"].describe())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of bilirubin values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(df[\"blood(mg/dL)\"], bins=40, edgecolor='black', alpha=0.7, color='#2196F3')\n",
    "axes[0].set_xlabel(\"Bilirubin (mg/dL)\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Distribution of Serum Bilirubin\")\n",
    "axes[0].axvline(df[\"blood(mg/dL)\"].mean(), color='red', linestyle='--', label=f'Mean: {df[\"blood(mg/dL)\"].mean():.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].boxplot(df[\"blood(mg/dL)\"], vert=True)\n",
    "axes[1].set_ylabel(\"Bilirubin (mg/dL)\")\n",
    "axes[1].set_title(\"Bilirubin Box Plot\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Extract MedSigLIP Embeddings\n",
    "\n",
    "We use the frozen MedSigLIP vision encoder to extract 1152-dimensional embeddings\n",
    "for each neonatal image. These embeddings are cached for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to images that exist on disk\n",
    "image_paths = []\n",
    "bilirubin_values = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    img_path = IMAGES_DIR / row[\"image_idx\"]\n",
    "    if img_path.exists():\n",
    "        image_paths.append(img_path)\n",
    "        bilirubin_values.append(float(row[\"blood(mg/dL)\"]))\n",
    "\n",
    "bilirubin_values = np.array(bilirubin_values, dtype=np.float32)\n",
    "print(f\"Valid image-label pairs: {len(image_paths)}\")\n",
    "print(f\"Bilirubin range: {bilirubin_values.min():.1f} - {bilirubin_values.max():.1f} mg/dL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = Path(\"../models/linear_probes\")\n",
    "EMB_CACHE = CACHE_DIR / \"jaundice_regression_embeddings.npy\"\n",
    "\n",
    "if EMB_CACHE.exists():\n",
    "    print(\"Loading cached embeddings...\")\n",
    "    embeddings = np.load(EMB_CACHE)\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "else:\n",
    "    print(\"Extracting MedSigLIP embeddings (requires HF_TOKEN)...\")\n",
    "    from train_linear_probes import EmbeddingExtractor\n",
    "    extractor = EmbeddingExtractor()\n",
    "    embeddings = extractor.extract_batch_embeddings(image_paths, batch_size=8)\n",
    "    CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    np.save(EMB_CACHE, embeddings)\n",
    "    print(f\"Saved embeddings: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Baseline: Color-Based Bilirubin Estimation\n",
    "\n",
    "Before training the ML model, we evaluate the existing color-based estimator\n",
    "which uses a simple yellow-index formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def estimate_bilirubin_color(img_path):\n",
    "    \"\"\"Color-based bilirubin estimate (yellow-index formula).\"\"\"\n",
    "    img = np.array(Image.open(img_path).convert(\"RGB\")).astype(float)\n",
    "    r, g, b = img[:,:,0], img[:,:,1], img[:,:,2]\n",
    "    yellow_index = (r + g - b) / (r + g + b + 1e-6)\n",
    "    return max(0, (np.mean(yellow_index) - 0.2) * 50)\n",
    "\n",
    "# Evaluate color-based method on a sample\n",
    "n_sample = min(200, len(image_paths))\n",
    "indices = np.random.RandomState(42).choice(len(image_paths), n_sample, replace=False)\n",
    "\n",
    "color_preds = []\n",
    "color_actuals = []\n",
    "for idx in indices:\n",
    "    pred = estimate_bilirubin_color(image_paths[idx])\n",
    "    color_preds.append(pred)\n",
    "    color_actuals.append(bilirubin_values[idx])\n",
    "\n",
    "color_preds = np.array(color_preds)\n",
    "color_actuals = np.array(color_actuals)\n",
    "\n",
    "color_mae = np.abs(color_preds - color_actuals).mean()\n",
    "color_rmse = np.sqrt(np.mean((color_preds - color_actuals)**2))\n",
    "color_r, _ = stats.pearsonr(color_preds, color_actuals)\n",
    "\n",
    "print(\"=== Color-Based Baseline ===\")\n",
    "print(f\"MAE:       {color_mae:.3f} mg/dL\")\n",
    "print(f\"RMSE:      {color_rmse:.3f} mg/dL\")\n",
    "print(f\"Pearson r: {color_r:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Train Bilirubin Regression Head\n",
    "\n",
    "Architecture: `Linear(D, 256) -> ReLU -> Dropout(0.3) -> Linear(256, 1)`\n",
    "\n",
    "- **Loss**: Huber loss (robust to outliers)\n",
    "- **Optimizer**: Adam with weight decay\n",
    "- **Scheduler**: ReduceLROnPlateau\n",
    "- **Early stopping**: patience=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilirubinRegressorHead(nn.Module):\n",
    "    \"\"\"2-layer MLP regression head.\"\"\"\n",
    "    def __init__(self, input_dim=1152, hidden_dim=256, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "# Data split: 70/15/15\n",
    "SEED = 42\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    embeddings, bilirubin_values, test_size=0.15, random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.15/0.85, random_state=SEED)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "print(f\"Embedding dim: {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_dim = embeddings.shape[1]\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_t = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "model = BilirubinRegressorHead(input_dim=input_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "criterion = nn.HuberLoss(delta=2.0)\n",
    "\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "PATIENCE = 15\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_state = None\n",
    "patience_ctr = 0\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"val_mae\": []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    indices = torch.randperm(len(X_train_t))\n",
    "    epoch_loss, n_batches = 0.0, 0\n",
    "    for i in range(0, len(indices), BATCH_SIZE):\n",
    "        batch_idx = indices[i:i+BATCH_SIZE]\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_train_t[batch_idx])\n",
    "        loss = criterion(pred, y_train_t[batch_idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    avg_train = epoch_loss / max(n_batches, 1)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(X_val_t)\n",
    "        val_loss = criterion(val_pred, y_val_t).item()\n",
    "        val_mae = torch.abs(val_pred - y_val_t).mean().item()\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    history[\"train_loss\"].append(avg_train)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_mae\"].append(val_mae)\n",
    "\n",
    "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Train: {avg_train:.4f} | Val: {val_loss:.4f} | Val MAE: {val_mae:.2f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = model.state_dict().copy()\n",
    "        patience_ctr = 0\n",
    "    else:\n",
    "        patience_ctr += 1\n",
    "        if patience_ctr >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "if best_state:\n",
    "    model.load_state_dict(best_state)\n",
    "print(f\"\\nBest val loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history[\"train_loss\"], label=\"Train\", alpha=0.8)\n",
    "axes[0].plot(history[\"val_loss\"], label=\"Validation\", alpha=0.8)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Huber Loss\")\n",
    "axes[0].set_title(\"Training & Validation Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history[\"val_mae\"], color='green', alpha=0.8)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"MAE (mg/dL)\")\n",
    "axes[1].set_title(\"Validation MAE\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred = model(X_test_t).cpu().numpy()\n",
    "    test_actual = y_test_t.cpu().numpy()\n",
    "\n",
    "ml_mae = np.abs(test_pred - test_actual).mean()\n",
    "ml_rmse = np.sqrt(np.mean((test_pred - test_actual)**2))\n",
    "ml_r, ml_p = stats.pearsonr(test_pred, test_actual)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TEST SET RESULTS — MedSigLIP Regressor\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"MAE:       {ml_mae:.3f} mg/dL\")\n",
    "print(f\"RMSE:      {ml_rmse:.3f} mg/dL\")\n",
    "print(f\"Pearson r: {ml_r:.4f} (p={ml_p:.2e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 7. Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 7a. Predicted vs Actual scatter\n",
    "ax = axes[0]\n",
    "ax.scatter(test_actual, test_pred, alpha=0.4, s=15, color='#2196F3')\n",
    "lims = [min(test_actual.min(), test_pred.min()), max(test_actual.max(), test_pred.max())]\n",
    "ax.plot(lims, lims, 'r--', linewidth=1.5, label='Perfect prediction')\n",
    "ax.set_xlabel(\"Actual Bilirubin (mg/dL)\")\n",
    "ax.set_ylabel(\"Predicted Bilirubin (mg/dL)\")\n",
    "ax.set_title(f\"Predicted vs Actual (r={ml_r:.3f})\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 7b. Bland-Altman plot\n",
    "ax = axes[1]\n",
    "diff = test_pred - test_actual\n",
    "mean_vals = (test_pred + test_actual) / 2\n",
    "mean_diff = np.mean(diff)\n",
    "std_diff = np.std(diff)\n",
    "loa_upper = mean_diff + 1.96 * std_diff\n",
    "loa_lower = mean_diff - 1.96 * std_diff\n",
    "\n",
    "ax.scatter(mean_vals, diff, alpha=0.4, s=15, color='#4CAF50')\n",
    "ax.axhline(mean_diff, color='red', linestyle='-', label=f'Mean: {mean_diff:.2f}')\n",
    "ax.axhline(loa_upper, color='orange', linestyle='--', label=f'+1.96 SD: {loa_upper:.2f}')\n",
    "ax.axhline(loa_lower, color='orange', linestyle='--', label=f'-1.96 SD: {loa_lower:.2f}')\n",
    "ax.set_xlabel(\"Mean of Predicted & Actual (mg/dL)\")\n",
    "ax.set_ylabel(\"Difference (Predicted - Actual)\")\n",
    "ax.set_title(\"Bland-Altman Plot\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 7c. Error distribution\n",
    "ax = axes[2]\n",
    "errors = test_pred - test_actual\n",
    "ax.hist(errors, bins=30, edgecolor='black', alpha=0.7, color='#FF9800')\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=1.5)\n",
    "ax.set_xlabel(\"Prediction Error (mg/dL)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(f\"Error Distribution (MAE={ml_mae:.2f})\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 8. Before vs After Comparison\n",
    "\n",
    "| Metric | Color-Based (Before) | MedSigLIP Regressor (After) |\n",
    "|--------|---------------------|-----------------------------|\n",
    "| MAE    | Higher              | Lower (trained on data)     |\n",
    "| RMSE   | Higher              | Lower                       |\n",
    "| Pearson r | Lower            | Higher (captures embeddings)|\n",
    "\n",
    "The MedSigLIP-based regressor leverages deep learned features from a medical vision\n",
    "model, capturing patterns invisible to simple color analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BEFORE vs AFTER COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<15} {'Color-Based':<20} {'MedSigLIP Regressor':<20}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'MAE (mg/dL)':<15} {color_mae:<20.3f} {ml_mae:<20.3f}\")\n",
    "print(f\"{'RMSE (mg/dL)':<15} {color_rmse:<20.3f} {ml_rmse:<20.3f}\")\n",
    "print(f\"{'Pearson r':<15} {color_r:<20.4f} {ml_r:<20.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if ml_mae < color_mae:\n",
    "    improvement = (1 - ml_mae / color_mae) * 100\n",
    "    print(f\"\\nMAE improvement: {improvement:.1f}% reduction with MedSigLIP regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 9. Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../models/linear_probes\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = output_dir / \"bilirubin_regressor.pt\"\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"input_dim\": input_dim,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"metrics\": {\n",
    "        \"mae\": round(float(ml_mae), 3),\n",
    "        \"rmse\": round(float(ml_rmse), 3),\n",
    "        \"pearson_r\": round(float(ml_r), 4),\n",
    "    },\n",
    "}, model_path)\n",
    "print(f\"Model saved: {model_path}\")\n",
    "\n",
    "# Save detailed results\n",
    "results = {\n",
    "    \"method\": \"MedSigLIP Bilirubin Regressor\",\n",
    "    \"hai_def_model\": \"google/medsiglip-448\",\n",
    "    \"architecture\": f\"Linear({input_dim},256)->ReLU->Dropout(0.3)->Linear(256,1)\",\n",
    "    \"loss\": \"HuberLoss(delta=2.0)\",\n",
    "    \"test_mae\": round(float(ml_mae), 3),\n",
    "    \"test_rmse\": round(float(ml_rmse), 3),\n",
    "    \"test_pearson_r\": round(float(ml_r), 4),\n",
    "    \"baseline_color_mae\": round(float(color_mae), 3),\n",
    "    \"baseline_color_rmse\": round(float(color_rmse), 3),\n",
    "    \"baseline_color_pearson_r\": round(float(color_r), 4),\n",
    "    \"bland_altman\": {\n",
    "        \"mean_diff\": round(float(mean_diff), 3),\n",
    "        \"std_diff\": round(float(std_diff), 3),\n",
    "        \"loa_upper\": round(float(loa_upper), 3),\n",
    "        \"loa_lower\": round(float(loa_lower), 3),\n",
    "    },\n",
    "    \"epochs_trained\": len(history[\"train_loss\"]),\n",
    "    \"test_size\": len(test_actual),\n",
    "}\n",
    "\n",
    "results_path = output_dir / \"bilirubin_regression_results.json\"\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Results saved: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates a **novel application** of the MedSigLIP HAI-DEF model:\n",
    "continuous bilirubin regression from neonatal skin images.\n",
    "\n",
    "**Key findings**:\n",
    "- Frozen MedSigLIP embeddings capture medically relevant features for bilirubin estimation\n",
    "- A lightweight 2-layer MLP trained on these embeddings outperforms simple color analysis\n",
    "- The Bland-Altman analysis shows the prediction spread and systematic bias\n",
    "- This approach enables CHWs to get quantitative bilirubin estimates from phone photos\n",
    "\n",
    "**Clinical relevance**: Non-invasive bilirubin screening can reduce the need for heel-prick\n",
    "blood draws in newborns and enable earlier detection of hyperbilirubinemia in\n",
    "resource-limited settings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}